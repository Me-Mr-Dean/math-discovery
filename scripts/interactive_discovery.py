#!/usr/bin/env python3
"""
Interactive Mathematical Pattern Discovery Engine - Main Script
============================================================

Interactive interface that makes it easy to discover patterns in datasets
generated by the Universal Dataset Generator or legacy datasets.

Usage:
    python scripts/interactive_discovery.py

Author: Mathematical Pattern Discovery Team
"""

import sys
import pandas as pd
import numpy as np
from pathlib import Path
import json
import time
from typing import List, Dict, Tuple, Optional
import warnings

warnings.filterwarnings("ignore")

# Add project src to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / "src"))

try:
    from core.discovery_engine import UniversalMathDiscovery
    from utils.path_utils import find_data_file, get_data_directory
    from analyzers.prime_analyzer import PurePrimeMLDiscovery
except ImportError as e:
    print(f"❌ Import Error: {e}")
    print("Make sure you're running from the project root.")
    print("Try installing: pip install -e .")

    # Try absolute imports as fallback
    try:
        sys.path.insert(0, str(project_root))
        from src.core.discovery_engine import UniversalMathDiscovery
        from src.utils.path_utils import find_data_file, get_data_directory
        from src.analyzers.prime_analyzer import PurePrimeMLDiscovery

        print("✅ Fallback imports successful")
    except ImportError as e2:
        print(f"❌ Fallback import also failed: {e2}")
        sys.exit(1)


class DatasetInfo:
    """Container for dataset information"""

    def __init__(self, path: Path):
        self.path = path
        self.name = path.stem
        self.rule_name = path.parent.name if path.parent.name != "raw" else "legacy"
        self.size_mb = path.stat().st_size / (1024 * 1024)
        self.metadata_path = (
            path.parent / f"{path.stem.replace('.csv', '_metadata.json')}"
        )
        self.metadata = self._load_metadata()

    def _load_metadata(self) -> Dict:
        """Load metadata if available"""
        if self.metadata_path.exists():
            try:
                with open(self.metadata_path, "r") as f:
                    return json.load(f)
            except:
                pass
        return {}

    def get_shape(self) -> Tuple[int, int]:
        """Get dataset shape efficiently"""
        try:
            # Fast method - just count lines and read headers
            with open(self.path, "r") as f:
                header = f.readline()
                col_count = len(header.split(","))

            # Count total lines
            total_lines = sum(1 for _ in open(self.path))
            row_count = total_lines - 1  # Subtract header

            return (row_count, col_count)
        except:
            return (0, 0)

    def get_processor_type(self) -> str:
        """Determine the processor type from filename"""
        name_lower = self.name.lower()
        if "prefix" in name_lower and "suffix" in name_lower:
            return "Prefix-Suffix Matrix"
        elif "digit_tensor" in name_lower:
            return "Digit Tensor"
        elif "sequence_patterns" in name_lower:
            return "Sequence Patterns"
        elif "algebraic_features" in name_lower:
            return "Algebraic Features"
        elif "ml_dataset" in name_lower:
            return "Legacy ML Dataset"
        else:
            return "Unknown"


class InteractiveDiscoveryEngine:
    """Interactive interface for mathematical pattern discovery"""

    def __init__(self):
        try:
            self.data_dir = get_data_directory()
            self.output_dir = self.data_dir.parent / "output"
        except:
            # Fallback if path utils don't work
            self.data_dir = Path("data/raw")
            self.output_dir = Path("data/output")

        self.discovered_datasets = []
        self.results_history = []

    def scan_for_datasets(self) -> List[DatasetInfo]:
        """Scan for available datasets from Universal Generator and legacy sources"""
        print("🔍 Scanning for available datasets...")

        datasets = []

        # Scan Universal Generator outputs
        if self.output_dir.exists():
            for rule_dir in self.output_dir.iterdir():
                if rule_dir.is_dir():
                    for csv_file in rule_dir.glob("*.csv"):
                        # Skip metadata files
                        if "metadata" not in csv_file.name:
                            datasets.append(DatasetInfo(csv_file))

        # Scan legacy data directory
        if self.data_dir.exists():
            for csv_file in self.data_dir.glob("*.csv"):
                # Skip if already found in output dir
                if not any(d.path.name == csv_file.name for d in datasets):
                    datasets.append(DatasetInfo(csv_file))

        # Sort by rule name, then processor type
        datasets.sort(key=lambda d: (d.rule_name, d.get_processor_type(), d.name))

        self.discovered_datasets = datasets
        return datasets

    def display_available_datasets(self, datasets: List[DatasetInfo]) -> None:
        """Display available datasets in a nice format"""
        print(f"\n📊 DISCOVERED DATASETS ({len(datasets)} found)")
        print("=" * 80)

        if not datasets:
            print("❌ No datasets found!")
            print("\n💡 Generate datasets first:")
            print("   python src/generators/universal_generator.py interactive")
            print("   python src/generators/universal_generator.py demo")
            print("   python src/generators/prime_generator.py ml 10000")
            return

        # Group by rule
        by_rule = {}
        for dataset in datasets:
            rule = dataset.rule_name
            if rule not in by_rule:
                by_rule[rule] = []
            by_rule[rule].append(dataset)

        index = 1
        for rule_name, rule_datasets in by_rule.items():
            print(f"\n🎯 {rule_name.replace('_', ' ').title()}")
            print("-" * 50)

            for dataset in rule_datasets:
                shape = dataset.get_shape()
                processor_type = dataset.get_processor_type()

                print(f"  {index:2d}. {processor_type}")
                print(f"      📄 {dataset.name}")
                print(
                    f"      📊 Shape: {shape[0]:,} rows × {shape[1]} cols ({dataset.size_mb:.1f} MB)"
                )

                # Show metadata if available
                if dataset.metadata:
                    if "description" in dataset.metadata:
                        print(f"      📝 {dataset.metadata['description']}")

                print()
                index += 1

    def select_datasets(self, datasets: List[DatasetInfo]) -> List[DatasetInfo]:
        """Interactive dataset selection"""
        print("\n🎯 DATASET SELECTION")
        print("=" * 30)
        print("Choose how to proceed:")
        print()
        print("  1. Analyze single dataset")
        print("  2. Analyze all datasets for one rule")
        print("  3. Analyze all datasets (comprehensive)")
        print("  4. Custom selection")
        print("  5. Auto-select best datasets")
        print()

        try:
            choice = input("Enter your choice (1-5): ").strip()

            if choice == "1":
                return self._select_single_dataset(datasets)
            elif choice == "2":
                return self._select_by_rule(datasets)
            elif choice == "3":
                print("⚠️  This will analyze ALL datasets. This may take a while.")
                confirm = input("Continue? (y/N): ").strip().lower()
                if confirm == "y":
                    return datasets
                else:
                    return []
            elif choice == "4":
                return self._custom_selection(datasets)
            elif choice == "5":
                return self._auto_select_best(datasets)
            else:
                print("❌ Invalid choice")
                return []

        except KeyboardInterrupt:
            print("\n❌ Selection cancelled")
            return []
        except Exception as e:
            print(f"❌ Error in selection: {e}")
            return []

    def _select_single_dataset(self, datasets: List[DatasetInfo]) -> List[DatasetInfo]:
        """Select a single dataset"""
        print("\nEnter the dataset number: ", end="")
        try:
            num = int(input().strip())
            if 1 <= num <= len(datasets):
                selected = datasets[num - 1]
                print(
                    f"✅ Selected: {selected.rule_name} - {selected.get_processor_type()}"
                )
                return [selected]
            else:
                print("❌ Invalid dataset number")
                return []
        except ValueError:
            print("❌ Please enter a valid number")
            return []

    def _select_by_rule(self, datasets: List[DatasetInfo]) -> List[DatasetInfo]:
        """Select all datasets for one rule"""
        # Get unique rules
        rules = {}
        for dataset in datasets:
            rule = dataset.rule_name
            if rule not in rules:
                rules[rule] = []
            rules[rule].append(dataset)

        print("\nAvailable rules:")
        rule_list = list(rules.keys())
        for i, rule in enumerate(rule_list, 1):
            count = len(rules[rule])
            print(f"  {i}. {rule.replace('_', ' ').title()} ({count} datasets)")

        print("\nEnter rule number: ", end="")
        try:
            num = int(input().strip())
            if 1 <= num <= len(rule_list):
                selected_rule = rule_list[num - 1]
                selected_datasets = rules[selected_rule]
                print(
                    f"✅ Selected {len(selected_datasets)} datasets for: {selected_rule}"
                )
                return selected_datasets
            else:
                print("❌ Invalid rule number")
                return []
        except ValueError:
            print("❌ Please enter a valid number")
            return []

    def _custom_selection(self, datasets: List[DatasetInfo]) -> List[DatasetInfo]:
        """Custom dataset selection"""
        print("\nEnter dataset numbers (comma-separated): ", end="")
        try:
            numbers = input().strip().split(",")
            selected = []

            for num_str in numbers:
                num = int(num_str.strip())
                if 1 <= num <= len(datasets):
                    selected.append(datasets[num - 1])
                else:
                    print(f"⚠️  Skipping invalid number: {num}")

            if selected:
                print(f"✅ Selected {len(selected)} datasets")
                for dataset in selected:
                    print(f"   • {dataset.rule_name} - {dataset.get_processor_type()}")

            return selected

        except ValueError:
            print("❌ Please enter valid numbers")
            return []

    def _auto_select_best(self, datasets: List[DatasetInfo]) -> List[DatasetInfo]:
        """Auto-select best datasets for analysis"""
        print("\n🤖 Auto-selecting best datasets...")

        selected = []

        # Group by rule
        by_rule = {}
        for dataset in datasets:
            rule = dataset.rule_name
            if rule not in by_rule:
                by_rule[rule] = []
            by_rule[rule].append(dataset)

        for rule_name, rule_datasets in by_rule.items():
            # Selection criteria:
            # 1. Prefer algebraic_features (most comprehensive)
            # 2. If no algebraic_features, prefer digit_tensor or sequence_patterns
            # 3. Limit to reasonable size (< 100MB each)
            best = None
            for dataset in rule_datasets:
                if dataset.size_mb > 100:  # Skip very large datasets
                    continue

                if "algebraic_features" in dataset.name:
                    best = dataset
                    break
                elif (
                    "digit_tensor" in dataset.name
                    or "sequence_patterns" in dataset.name
                ):
                    if best is None:
                        best = dataset

            if best:
                selected.append(best)

            # Limit to 5 datasets total
            if len(selected) >= 5:
                break

        if selected:
            print(f"✅ Auto-selected {len(selected)} datasets:")
            for dataset in selected:
                print(f"   • {dataset.rule_name} - {dataset.get_processor_type()}")
        else:
            print("❌ No suitable datasets found for auto-selection")

        return selected

    def configure_analysis(self) -> Dict:
        """Configure analysis parameters"""
        print("\n⚙️  ANALYSIS CONFIGURATION")
        print("=" * 30)

        config = {
            "max_samples": 50000,
            "embedding": None,
            "quick_mode": False,
            "save_results": True,
            "comparative_analysis": True,
        }

        print("Choose analysis mode:")
        print("  1. Quick analysis (faster, smaller samples)")
        print("  2. Standard analysis (balanced)")
        print("  3. Deep analysis (slower, more thorough)")
        print("  4. Custom configuration")
        print()

        try:
            mode = input("Enter mode (1-4, default: 2): ").strip()
            if not mode:
                mode = "2"

            if mode == "1":
                config["max_samples"] = 10000
                config["quick_mode"] = True
                print("✅ Quick analysis mode selected")

            elif mode == "3":
                config["max_samples"] = 100000
                config["embedding"] = "fourier"
                print("✅ Deep analysis mode selected")

            elif mode == "4":
                print("\nCustom configuration:")

                # Max samples
                samples = input(
                    f"Max samples (default: {config['max_samples']}): "
                ).strip()
                if samples:
                    config["max_samples"] = int(samples)

                # Embedding
                embedding = input(
                    "Use embeddings? (fourier/pca/none, default: none): "
                ).strip()
                if embedding in ["fourier", "pca"]:
                    config["embedding"] = embedding

                # Save results
                save = input("Save results? (Y/n): ").strip().lower()
                config["save_results"] = save != "n"

                print("✅ Custom configuration set")
            else:
                print("✅ Standard analysis mode selected")

        except ValueError:
            print("⚠️  Invalid input, using default configuration")
        except KeyboardInterrupt:
            print("\n❌ Configuration cancelled")
            return None

        return config

    def analyze_dataset(self, dataset: DatasetInfo, config: Dict) -> Dict:
        """Analyze a single dataset"""
        print(f"\n🔬 ANALYZING: {dataset.rule_name} - {dataset.get_processor_type()}")
        print("=" * 60)

        try:
            # Load the dataset
            print("📊 Loading dataset...")

            # Smart loading based on size
            if dataset.size_mb > 100:
                print(f"⚠️  Large dataset ({dataset.size_mb:.1f} MB), using sample...")
                df = pd.read_csv(dataset.path, nrows=config["max_samples"])
            else:
                df = pd.read_csv(dataset.path, index_col=0)

            # Limit samples if needed
            if len(df) > config["max_samples"]:
                print(
                    f"🎯 Sampling {config['max_samples']:,} rows from {len(df):,} total"
                )
                df = df.sample(n=config["max_samples"], random_state=42)

            print(f"📈 Dataset shape: {df.shape}")
            print(
                f"📝 Columns: {list(df.columns)[:10]}{'...' if len(df.columns) > 10 else ''}"
            )

            # Determine target column
            target_col = None
            if "target" in df.columns:
                target_col = "target"
            elif any(col.endswith("_target") for col in df.columns):
                target_col = next(col for col in df.columns if col.endswith("_target"))

            if target_col is None:
                print("🧮 Matrix dataset detected - analyzing patterns...")
                result = self._analyze_matrix_dataset(df, dataset, config)
            else:
                print("🤖 ML dataset detected - running discovery engine...")
                result = self._analyze_ml_dataset(df, target_col, dataset, config)

            return result

        except Exception as e:
            print(f"❌ Analysis failed: {e}")
            return {"error": str(e), "dataset": dataset.name}

    def _analyze_matrix_dataset(
        self, df: pd.DataFrame, dataset: DatasetInfo, config: Dict
    ) -> Dict:
        """Analyze prefix-suffix matrix datasets"""
        # Basic statistics
        total_cells = df.shape[0] * df.shape[1]
        positive_cells = (df == 1).sum().sum()
        positive_rate = positive_cells / total_cells if total_cells > 0 else 0

        # Row and column statistics
        row_stats = df.sum(axis=1).describe()
        col_stats = df.sum(axis=0).describe()

        # Find best patterns
        best_rows = df.sum(axis=1).nlargest(5)
        best_cols = df.sum(axis=0).nlargest(5)

        result = {
            "dataset_name": dataset.name,
            "dataset_type": "matrix",
            "shape": df.shape,
            "total_cells": total_cells,
            "positive_cells": int(positive_cells),
            "positive_rate": positive_rate,
            "row_stats": row_stats.to_dict(),
            "col_stats": col_stats.to_dict(),
            "best_rows": best_rows.to_dict(),
            "best_cols": best_cols.to_dict(),
            "analysis_time": time.time(),
        }

        print(
            f"📊 Positive rate: {positive_rate:.4f} ({positive_cells:,}/{total_cells:,})"
        )
        print(
            f"🏆 Best row (prefix): {best_rows.index[0]} ({best_rows.iloc[0]} matches)"
        )
        print(
            f"🏆 Best column (suffix): {best_cols.index[0]} ({best_cols.iloc[0]} matches)"
        )

        return result

    def _analyze_ml_dataset(
        self, df: pd.DataFrame, target_col: str, dataset: DatasetInfo, config: Dict
    ) -> Dict:
        """Analyze ML-ready datasets using the discovery engine"""
        print("🤖 Running ML analysis...")

        # Extract features and targets
        feature_cols = [col for col in df.columns if col != target_col]
        X = df[feature_cols]
        y = df[target_col]

        positive_rate = y.mean()
        print(f"📊 Positive rate: {positive_rate:.4f} ({y.sum():,}/{len(y):,})")

        if positive_rate == 0 or positive_rate == 1:
            print("⚠️  No variation in target - skipping ML analysis")
            return {
                "dataset_name": dataset.name,
                "dataset_type": "ml",
                "error": "No variation in target variable",
                "positive_rate": positive_rate,
            }

        try:
            print("🔧 Initializing discovery engine...")

            # Create a synthetic target function for the discovery engine
            def synthetic_target(n):
                return n % 2 == 0  # Placeholder

            # Use a reasonable scope for analysis
            max_num = min(config["max_samples"], 10000)

            discoverer = UniversalMathDiscovery(
                target_function=synthetic_target,
                function_name=f"Analysis of {dataset.rule_name}",
                max_number=max_num,
                embedding=config.get("embedding"),
            )

            # Override the data with our loaded dataset
            print("📊 Running pattern discovery...")
            discoverer.X = X
            discoverer.y = y.values
            discoverer.feature_names = feature_cols

            # Train models directly
            from sklearn.model_selection import train_test_split
            from sklearn.ensemble import (
                RandomForestClassifier,
                GradientBoostingClassifier,
            )
            from sklearn.linear_model import LogisticRegression
            from sklearn.preprocessing import StandardScaler
            from sklearn.metrics import roc_auc_score

            # Split data
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2, random_state=42, stratify=y
            )

            # Scale features
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_test_scaled = scaler.transform(X_test)

            # Train models
            models = {}
            models_config = {
                "logistic": LogisticRegression(random_state=42, max_iter=1000),
                "random_forest": RandomForestClassifier(
                    n_estimators=100, random_state=42, max_depth=10, n_jobs=-1
                ),
                "gradient_boost": GradientBoostingClassifier(
                    n_estimators=50, random_state=42, max_depth=6
                ),
            }

            print("🤖 Training models...")
            for name, model in models_config.items():
                if name == "logistic":
                    model.fit(X_train_scaled, y_train)
                    train_pred = model.predict(X_train_scaled)
                    test_pred = model.predict(X_test_scaled)
                    test_proba = model.predict_proba(X_test_scaled)[:, 1]
                else:
                    model.fit(X_train, y_train)
                    train_pred = model.predict(X_train)
                    test_pred = model.predict(X_test)
                    test_proba = model.predict_proba(X_test)[:, 1]

                train_acc = (train_pred == y_train).mean()
                test_acc = (test_pred == y_test).mean()
                test_auc = roc_auc_score(y_test, test_proba)

                models[name] = {
                    "model": model,
                    "train_accuracy": train_acc,
                    "test_accuracy": test_acc,
                    "test_auc": test_auc,
                }

            # Analyze feature importance
            best_model = models["random_forest"]["model"]
            if hasattr(best_model, "feature_importances_"):
                importances = best_model.feature_importances_
                feature_importance = pd.DataFrame(
                    {"feature": feature_cols, "importance": importances}
                ).sort_values("importance", ascending=False)
                top_features = feature_importance.head(10).to_dict("records")
            else:
                top_features = []

            result = {
                "dataset_name": dataset.name,
                "dataset_type": "ml",
                "shape": df.shape,
                "positive_rate": positive_rate,
                "feature_count": len(feature_cols),
                "model_performance": {
                    name: {
                        "train_acc": info["train_accuracy"],
                        "test_acc": info["test_accuracy"],
                        "test_auc": info["test_auc"],
                    }
                    for name, info in models.items()
                },
                "top_features": top_features,
                "analysis_time": time.time(),
            }

            # Show results
            print("\n🏆 Model Performance:")
            for name, perf in result["model_performance"].items():
                print(
                    f"  {name}: Test Acc = {perf['test_acc']:.3f}, AUC = {perf['test_auc']:.3f}"
                )

            if result["top_features"]:
                print("\n🎯 Top Features:")
                for feat in result["top_features"][:5]:
                    print(f"  {feat['feature']}: {feat['importance']:.4f}")

            return result

        except Exception as e:
            print(f"❌ ML analysis failed: {e}")
            return {
                "dataset_name": dataset.name,
                "dataset_type": "ml",
                "error": str(e),
                "positive_rate": positive_rate,
            }

    def run_interactive_session(self) -> None:
        """Main interactive session"""
        print("🧮 INTERACTIVE MATHEMATICAL PATTERN DISCOVERY")
        print("=" * 60)
        print("Analyze datasets generated by the Universal Dataset Generator")
        print("and discover mathematical patterns automatically!")
        print()

        try:
            # Step 1: Scan for datasets
            datasets = self.scan_for_datasets()
            if not datasets:
                print("No datasets found. Generate some first!")
                print("\n💡 Quick start:")
                print("   python src/generators/universal_generator.py demo")
                return

            # Step 2: Display available datasets
            self.display_available_datasets(datasets)

            # Step 3: Select datasets
            selected_datasets = self.select_datasets(datasets)
            if not selected_datasets:
                print("❌ No datasets selected. Exiting.")
                return

            # Step 4: Configure analysis
            config = self.configure_analysis()
            if config is None:
                return

            # Step 5: Run analysis
            print(f"\n🚀 STARTING ANALYSIS")
            print("=" * 30)

            results = []
            start_time = time.time()

            for i, dataset in enumerate(selected_datasets, 1):
                print(f"\n[{i}/{len(selected_datasets)}] Processing: {dataset.name}")
                result = self.analyze_dataset(dataset, config)
                results.append(result)

            total_time = time.time() - start_time

            # Step 6: Save results and summary
            if config.get("save_results", True):
                output_file = Path("interactive_discovery_results.json")
                with open(output_file, "w") as f:
                    json.dump(
                        {
                            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                            "config": config,
                            "datasets_analyzed": len(selected_datasets),
                            "total_time": total_time,
                            "results": results,
                        },
                        f,
                        indent=2,
                        default=str,
                    )
                print(f"\n💾 Results saved to: {output_file}")

            # Generate summary
            print(f"\n📊 ANALYSIS COMPLETE!")
            print("=" * 30)
            print(f"⏱️  Total time: {total_time:.1f}s")
            print(f"📊 Datasets analyzed: {len(selected_datasets)}")

            successful = len([r for r in results if "error" not in r])
            print(f"✅ Successful analyses: {successful}/{len(results)}")

            # Show best results
            ml_results = [
                r
                for r in results
                if r.get("dataset_type") == "ml" and "model_performance" in r
            ]
            if ml_results:
                print(f"\n🏆 Best ML Results:")
                for result in ml_results:
                    name = result["dataset_name"]
                    best_acc = max(
                        (
                            perf["test_acc"]
                            for perf in result["model_performance"].values()
                        ),
                        default=0,
                    )
                    print(f"  {name}: {best_acc:.3f} accuracy")

            print(f"\n✨ Interactive discovery session complete!")

        except KeyboardInterrupt:
            print("\n❌ Session interrupted by user")
        except Exception as e:
            print(f"\n💥 Session failed: {e}")
            import traceback

            traceback.print_exc()


def main():
    """Main entry point"""
    engine = InteractiveDiscoveryEngine()
    engine.run_interactive_session()


if __name__ == "__main__":
    main()
